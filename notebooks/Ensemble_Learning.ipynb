{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Methods for Leukemia Classification using Nested Cross-Validation\n",
    "\n",
    "This notebook demonstrates how ensemble methods, specifically **blending** (soft voting) and **stacking**, can improve classification performance compared to individual models on a high-dimensional gene-expression dataset. We use the classic **Golub leukemia dataset** and employ **nested cross-validation** for robust performance evaluation and hyperparameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 | Objectives & Road-map\n",
    "\n",
    "*   **Task:** Classify leukemia sub-types (ALL vs. AML) from genome-wide mRNA expression data.\n",
    "*   **Data:** Golub et al., 1999 – 72 patients × 7,129 gene probes.\n",
    "*   **Demonstration Goal:** Show that carefully tuned blending and stacking ensembles, assessed with nested CV, provide a statistically significant performance lift over the best single, tuned model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 | Environment Setup\n",
    "\n",
    "Import necessary libraries for data handling, modeling, evaluation, and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "# Data fetching\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "# Preprocessing and Feature Selection\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import VarianceThreshold, SelectKBest, f_classif\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA # For EDA\n",
    "\n",
    "# Model Selection and Evaluation\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV, cross_validate\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, make_scorer\n",
    "\n",
    "# Base Learners\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "# Ensemble Methods\n",
    "from sklearn.ensemble import VotingClassifier, StackingClassifier\n",
    "\n",
    "# Statistical Testing\n",
    "from scipy import stats\n",
    "\n",
    "# Configure plots and warnings\n",
    "%matplotlib inline\n",
    "sns.set_style('whitegrid')\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning) # Ignore common warnings from sklearn/seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 | Load the Golub Gene-Expression Data\n",
    "\n",
    "We fetch the data directly from OpenML using its `data_id`. The original dataset has multiple subtypes of Acute Lymphoblastic Leukemia (ALL). For this demonstration, we simplify the task to a binary classification problem: ALL vs. Acute Myeloid Leukemia (AML)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch data (features X, target y)\n",
    "# Setting parser='liac-arff' to avoid potential loading issues\n",
    "X, y = fetch_openml(data_id=1104, as_frame=False, return_X_y=True, parser='liac-arff')\n",
    "\n",
    "# Original shape\n",
    "print(f\"Original data shape: {X.shape}\") # (samples, features)\n",
    "\n",
    "# Binarize the target variable: Combine ALL subtypes\n",
    "y = np.array([ 'ALL' if label in ['allB', 'allT'] else 'AML' for label in y ])\n",
    "\n",
    "# Display class distribution\n",
    "print(\"\\nClass distribution after binarization:\")\n",
    "unique, counts = np.unique(y, return_counts=True)\n",
    "class_dist = dict(zip(unique, counts))\n",
    "print(class_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 | Quick EDA & Sanity Checks\n",
    "\n",
    "Let's perform some basic exploratory data analysis:\n",
    "1.  **Class Distribution:** Check if the classes are reasonably balanced.\n",
    "2.  **Feature Variance:** Examine the variance of gene expression values. Many genes might have very low variance across patients, indicating they might not be informative.\n",
    "3.  **PCA Visualization:** Use Principal Component Analysis (PCA) to reduce dimensionality to 2D and visualize potential separation between the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Class Distribution (already printed above)\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.countplot(x=y)\n",
    "plt.title('Class Distribution (ALL vs AML)')\n",
    "plt.xlabel('Leukemia Type')\n",
    "plt.ylabel('Number of Patients')\n",
    "plt.show()\n",
    "print(f\"Class balance: {class_dist['ALL'] / len(y):.2f} ALL, {class_dist['AML'] / len(y):.2f} AML\")\n",
    "\n",
    "# 2. Feature Variance Histogram\n",
    "variances = np.var(X, axis=0)\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.hist(variances, bins=50, log=True)\n",
    "plt.title('Histogram of Gene Probe Variances (Log Scale)')\n",
    "plt.xlabel('Variance')\n",
    "plt.ylabel('Frequency (Log)')\n",
    "plt.show()\n",
    "print(f\"Number of features with zero variance: {np.sum(variances == 0)}\")\n",
    "print(f\"Median variance: {np.median(variances):.4f}\")\n",
    "\n",
    "# 3. PCA Scatter Plot\n",
    "# Scale data first for PCA\n",
    "scaler_eda = StandardScaler()\n",
    "X_scaled_eda = scaler_eda.fit_transform(X)\n",
    "\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "X_pca = pca.fit_transform(X_scaled_eda)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "for leukemia_type in ['ALL', 'AML']:\n",
    "    plt.scatter(X_pca[y == leukemia_type, 0],\n",
    "                X_pca[y == leukemia_type, 1],\n",
    "                label=leukemia_type,\n",
    "                alpha=0.7)\n",
    "plt.title('PCA of Golub Data (First 2 Components)')\n",
    "plt.xlabel(f'Principal Component 1 ({pca.explained_variance_ratio_[0]*100:.1f}% variance)')\n",
    "plt.ylabel(f'Principal Component 2 ({pca.explained_variance_ratio_[1]*100:.1f}% variance)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nEDA Findings:\")\n",
    "print(\"- Classes are somewhat imbalanced (approx 2/3 ALL, 1/3 AML). Stratified sampling is important.\")\n",
    "print(\"- Many gene probes have very low variance, supporting the use of VarianceThreshold.\")\n",
    "print(\"- PCA shows some separation between classes, suggesting classification is feasible.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 | Pre-processing Pipeline for p \u226b n Genomics\n",
    "\n",
    "High-dimensional data (p=7129 features >> n=72 samples) is prone to overfitting and the curse of dimensionality. We define a standard preprocessing pipeline:\n",
    "\n",
    "1.  **`VarianceThreshold`**: Removes features with zero variance.\n",
    "2.  **`SelectKBest`**: Selects the top `k` features most correlated with the target variable using the ANOVA F-statistic (`f_classif`). We choose `k=500` as a balance between retaining signal and reducing dimensionality.\n",
    "3.  **`StandardScaler`**: Standardizes features by removing the mean and scaling to unit variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = Pipeline([\n",
    "    ('vt', VarianceThreshold(threshold=0.0)),        # Drop zero-variance probes\n",
    "    ('kbest', SelectKBest(f_classif, k=500)),        # Keep top 500 informative genes\n",
    "    ('scale', StandardScaler())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6 | Nested Cross-Validation Design\n",
    "\n",
    "To get an unbiased estimate of model generalization performance and avoid information leakage during hyperparameter tuning, we use nested cross-validation (CV).\n",
    "\n",
    "*   **Outer Loop:** Splits the data into K folds (e.g., 5). Each fold serves as a final test set *once*. The performance metrics reported are the average across these K test sets. This estimates the model's performance on unseen data.\n",
    "    *   We use `StratifiedKFold` to maintain class proportions in each fold, important given the class imbalance.\n",
    "*   **Inner Loop:** Performed *within each training split* of the outer loop. Its purpose is to find the best hyperparameters for the model using only the outer loop's training data. `GridSearchCV` is commonly used here.\n",
    "\n",
    "> **Why nested?** Tuning hyperparameters and evaluating the model on the *same* CV folds leads to optimistic bias. Nested CV ensures that the final evaluation is performed on data completely unseen during hyperparameter optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the outer cross-validation strategy\n",
    "OUTER_CV_SPLITS = 5\n",
    "SEED = 42 # for reproducibility\n",
    "\n",
    "outer_cv = StratifiedKFold(n_splits=OUTER_CV_SPLITS, shuffle=True, random_state=SEED)\n",
    "\n",
    "# Define the inner cross-validation strategy (used by GridSearchCV)\n",
    "INNER_CV_SPLITS = 3\n",
    "inner_cv = StratifiedKFold(n_splits=INNER_CV_SPLITS, shuffle=True, random_state=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7 | Candidate Base Models & Parameter Grids\n",
    "\n",
    "We select a few standard classifiers as base learners. For each, we define a hyperparameter grid to be searched by `GridSearchCV` in the inner loop. The parameter names in the grid must follow the convention `step_name__parameter_name`. Since our models will be inside a pipeline with the name `clf`, the parameter names become `clf__<param_name>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_models = {\n",
    "    'logreg': {\n",
    "        'model': LogisticRegression(max_iter=5000, random_state=SEED, solver='liblinear'), # liblinear good for smaller datasets\n",
    "        'grid': {'clf__C': [0.01, 0.1, 1, 10, 100]}\n",
    "    },\n",
    "    'svm': {\n",
    "        'model': SVC(kernel='linear', probability=True, random_state=SEED),\n",
    "        'grid': {'clf__C': [0.01, 0.1, 1, 10, 100]}\n",
    "    },\n",
    "    'rf': {\n",
    "        'model': RandomForestClassifier(random_state=SEED),\n",
    "        'grid': {'clf__n_estimators': [100, 200, 500],\n",
    "                 'clf__max_features': ['sqrt', 0.3],\n",
    "                 'clf__max_depth': [None, 10, 20]}\n",
    "    },\n",
    "    'gb': {\n",
    "        'model': GradientBoostingClassifier(random_state=SEED),\n",
    "        'grid': {'clf__n_estimators': [100, 200, 500],\n",
    "                 'clf__learning_rate': [0.01, 0.05, 0.1],\n",
    "                 'clf__max_depth': [3, 5]}\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8 | Ensemble Strategy #1 – Soft Blending (VotingClassifier)\n",
    "\n",
    "Blending averages the predictions (ideally probabilities, known as 'soft voting') of multiple base models. This often reduces variance and improves robustness.\n",
    "\n",
    "We will create a `VotingClassifier` using the *best performing* base models identified after running the initial nested CV. For simplicity in this setup, we will pre-select Logistic Regression, SVM, and Random Forest based on common performance characteristics, but a more rigorous approach might dynamically select the top N models based on the inner CV scores from the first phase.\n",
    "\n",
    "**Note:** Tuning hyperparameters *within* a VotingClassifier using GridSearchCV requires prefixing parameters with `estimatorname__parameter`. We will define the VotingClassifier using the base model *types* and let the `GridSearchCV` within the nested loop tune their parameters simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base estimators for the VotingClassifier\n",
    "# We use the same model instances defined earlier, but give them unique names for the ensemble\n",
    "clf1 = LogisticRegression(max_iter=5000, random_state=SEED, solver='liblinear')\n",
    "clf2 = SVC(kernel='linear', probability=True, random_state=SEED)\n",
    "clf3 = RandomForestClassifier(random_state=SEED)\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('logreg_vote', clf1), ('svm_vote', clf2), ('rf_vote', clf3)],\n",
    "    voting='soft' # Use predicted probabilities\n",
    ")\n",
    "\n",
    "# Define the parameter grid for tuning the models *inside* the VotingClassifier\n",
    "# Note the prefixes matching the estimator names above\n",
    "voting_grid = {\n",
    "    'clf__logreg_vote__C': [0.1, 1, 10],          # Reduced grid for speed\n",
    "    'clf__svm_vote__C': [0.1, 1, 10],             # Reduced grid for speed\n",
    "    'clf__rf_vote__n_estimators': [100, 200],    # Reduced grid for speed\n",
    "    'clf__rf_vote__max_features': ['sqrt', 0.3]\n",
    "    # We could also tune voting weights if desired, e.g., 'clf__weights': [...] \n",
    "}\n",
    "\n",
    "# Add to our dictionary of models to evaluate\n",
    "ensemble_models = {\n",
    "    'blend': {\n",
    "        'model': voting_clf,\n",
    "        'grid': voting_grid\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9 | Ensemble Strategy #2 – Stacking (level-1 meta-learner)\n",
    "\n",
    "Stacking uses the predictions of base models (level-0) as input features for a final meta-learner (level-1), which makes the final prediction. `StackingClassifier` handles the generation of these 'meta-features' internally using cross-validation to prevent leakage.\n",
    "\n",
    "We'll use the same base models as in blending and a `LogisticRegression` as the meta-learner. Similar to the `VotingClassifier`, we define the structure and the parameter grid for tuning the base learners *within* the stack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define base estimators for StackingClassifier (can be the same instances)\n",
    "# IMPORTANT: StackingClassifier clones estimators, so use the definitions\n",
    "estimators_stack = [\n",
    "    ('logreg_stack', LogisticRegression(max_iter=5000, random_state=SEED, solver='liblinear')),\n",
    "    ('svm_stack', SVC(kernel='linear', probability=True, random_state=SEED)),\n",
    "    ('rf_stack', RandomForestClassifier(random_state=SEED))\n",
    "]\n",
    "\n",
    "stacking_clf = StackingClassifier(\n",
    "    estimators=estimators_stack,\n",
    "    final_estimator=LogisticRegression(max_iter=2000), # Meta-learner\n",
    "    cv=inner_cv,           # Use same inner CV strategy for generating meta-features\n",
    "    stack_method='predict_proba', # Use probabilities as meta-features\n",
    "    passthrough=False      # Only use meta-features for final estimator\n",
    ")\n",
    "\n",
    "# Define the parameter grid for tuning models *inside* the StackingClassifier\n",
    "stacking_grid = {\n",
    "    'clf__logreg_stack__C': [0.1, 1, 10],         # Reduced grid\n",
    "    'clf__svm_stack__C': [0.1, 1, 10],            # Reduced grid\n",
    "    'clf__rf_stack__n_estimators': [100, 200],   # Reduced grid\n",
    "    'clf__rf_stack__max_features': ['sqrt'],\n",
    "    # We could also tune the final_estimator's parameters, e.g.:\n",
    "    'clf__final_estimator__C': [0.1, 1, 10]\n",
    "}\n",
    "\n",
    "# Add to our dictionary\n",
    "ensemble_models['stack'] = {\n",
    "    'model': stacking_clf,\n",
    "    'grid': stacking_grid\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10 | Running the Nested Loop & Collecting Scores\n",
    "\n",
    "Now we run the nested cross-validation loop for all base models and the two ensemble strategies.\n",
    "\n",
    "For each model:\n",
    "1.  Create a full pipeline: `preprocessing + classifier`.\n",
    "2.  Define the inner `GridSearchCV` to tune the hyperparameters defined in the grids.\n",
    "3.  Use `cross_validate` with the `outer_cv` strategy. It takes the `GridSearchCV` object as the estimator.\n",
    "    *   Inside each outer fold, `cross_validate` first calls `fit` on the `GridSearchCV` object using the outer *training* data. This triggers the inner CV loop to find the best parameters for that specific outer fold's training data.\n",
    "    *   `GridSearchCV` automatically refits a model using the *best found parameters* on the *entire* outer training set.\n",
    "    *   `cross_validate` then evaluates this refitted model on the outer *test* set.\n",
    "4.  We collect the test scores (ROC AUC) from each of the outer folds. We also `return_estimator=True` to potentially inspect the models trained on each outer fold later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the scoring metric (Area Under the ROC Curve, OvR for multi-class if needed)\n",
    "# For binary classification, 'roc_auc' is sufficient. needs_proba=True is crucial for AUC.\n",
    "metric = make_scorer(roc_auc_score, needs_proba=True)\n",
    "\n",
    "# Combine base models and ensembles for iteration\n",
    "all_models_to_run = {**base_models, **ensemble_models}\n",
    "\n",
    "nested_results = {}\n",
    "trained_estimators = {}\n",
    "\n",
    "print(f\"Starting Nested Cross-Validation (Outer folds={OUTER_CV_SPLITS}, Inner folds={INNER_CV_SPLITS})...\")\n",
    "\n",
    "for name, spec in all_models_to_run.items():\n",
    "    print(f\"\\nProcessing: {name}\")\n",
    "\n",
    "    # Create the full pipeline including preprocessing and the classifier\n",
    "    pipeline = Pipeline([\n",
    "        ('prep', preprocess),\n",
    "        ('clf', spec['model'])\n",
    "    ])\n",
    "\n",
    "    # Inner loop: GridSearchCV for hyperparameter tuning\n",
    "    # If grid is empty (e.g., simple avg voting), skip GridSearchCV\n",
    "    if spec['grid']:\n",
    "        inner_search = GridSearchCV(\n",
    "            estimator=pipeline,\n",
    "            param_grid=spec['grid'],\n",
    "            cv=inner_cv,\n",
    "            scoring=metric,\n",
    "            n_jobs=-1 # Use all available CPU cores\n",
    "        )\n",
    "    else:\n",
    "        # If no grid search needed, the pipeline itself is the estimator\n",
    "        inner_search = pipeline \n",
    "        print(f\"  (Skipping GridSearchCV for {name})\")\n",
    "\n",
    "    # Outer loop: cross_validate to get generalization score\n",
    "    try:\n",
    "      cv_output = cross_validate(\n",
    "          estimator=inner_search, # Pass the GridSearchCV object here\n",
    "          X=X,\n",
    "          y=y,\n",
    "          cv=outer_cv,\n",
    "          scoring=metric,\n",
    "          n_jobs=-1,\n",
    "          return_estimator=True, # Return the fitted estimator for each outer fold\n",
    "          error_score='raise' # See errors during CV\n",
    "      )\n",
    "\n",
    "      # Store results\n",
    "      nested_results[name] = cv_output['test_score']\n",
    "      trained_estimators[name] = cv_output['estimator'] # List of estimators, one per outer fold\n",
    "\n",
    "      print(f\"  Finished. Mean ROC AUC: {np.mean(cv_output['test_score']):.4f} (+/- {np.std(cv_output['test_score']):.4f})\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  ERROR processing {name}: {e}\")\n",
    "        nested_results[name] = np.array([np.nan] * OUTER_CV_SPLITS) # Fill with NaNs on error\n",
    "        trained_estimators[name] = [None] * OUTER_CV_SPLITS\n",
    "\n",
    "print(\"\\nNested Cross-Validation Complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11 | Visualising & Testing Performance Gains\n",
    "\n",
    "Now we visualize the distribution of ROC AUC scores obtained from the outer loop for each model using a box plot. This helps compare both the average performance and the variability.\n",
    "\n",
    "We then perform a statistical test to check if the performance difference between the best single model and the ensemble methods (specifically stacking, often the best performer) is statistically significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert results to DataFrame for easier plotting\n",
    "results_df = pd.DataFrame(nested_results)\n",
    "\n",
    "# Calculate mean scores for ranking\n",
    "mean_scores = results_df.mean().sort_values(ascending=False)\n",
    "print(\"\\nMean ROC AUC scores across outer folds:\")\n",
    "print(mean_scores)\n",
    "\n",
    "# Create box plot\n",
    "plt.figure(figsize=(12, 7))\n",
    "sns.boxplot(data=results_df[mean_scores.index], palette='viridis') # Order by mean score\n",
    "plt.title(f'Model Comparison: ROC AUC across {OUTER_CV_SPLITS} Outer Folds (Nested CV)')\n",
    "plt.ylabel('ROC AUC Score')\n",
    "plt.xlabel('Model')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistical Test: Compare best single model vs. stacking\n",
    "# Identify best single model (excluding ensembles)\n",
    "base_model_names = list(base_models.keys())\n",
    "best_single_model_name = results_df[base_model_names].mean().idxmax()\n",
    "best_single_model_scores = results_df[best_single_model_name]\n",
    "\n",
    "# Get stacking scores\n",
    "stacking_scores = results_df['stack']\n",
    "\n",
    "print(f\"\\nComparing best single model ('{best_single_model_name}') vs. Stacking Ensemble:\")\n",
    "\n",
    "# Check if scores are valid for testing\n",
    "if stacking_scores.isnull().any() or best_single_model_scores.isnull().any():\n",
    "    print(\"  Skipping statistical test due to NaN scores.\")\n",
    "else:\n",
    "    # Paired t-test (assumes normality, reasonable for AUC scores over ~5 folds)\n",
    "    # Alternatives: Wilcoxon signed-rank test (non-parametric)\n",
    "    t_stat, p_value_ttest = stats.ttest_rel(stacking_scores, best_single_model_scores)\n",
    "    print(f\"  Paired t-test: t-statistic = {t_stat:.3f}, p-value = {p_value_ttest:.4f}\")\n",
    "\n",
    "    # Wilcoxon signed-rank test\n",
    "    try:\n",
    "      # The wilcoxon test requires differences not all be zero. Check first.\n",
    "      diff = stacking_scores - best_single_model_scores\n",
    "      if np.all(diff == 0):\n",
    "          print(\"  Wilcoxon test not applicable: all score differences are zero.\")\n",
    "      else:\n",
    "          w_stat, p_value_wilcoxon = stats.wilcoxon(stacking_scores, best_single_model_scores)\n",
    "          print(f\"  Wilcoxon signed-rank test: W-statistic = {w_stat:.3f}, p-value = {p_value_wilcoxon:.4f}\")\n",
    "    except ValueError as e:\n",
    "        # Handle cases like zero differences if not caught above\n",
    "        print(f\"  Could not perform Wilcoxon test: {e}\")\n",
    "\n",
    "    # Interpretation (using alpha = 0.05)\n",
    "    alpha = 0.05\n",
    "    if p_value_ttest < alpha:\n",
    "        print(f\"  Conclusion (t-test): Stacking performance ({stacking_scores.mean():.4f}) is statistically significantly different from {best_single_model_name} ({best_single_model_scores.mean():.4f}) (p < {alpha}).\")\n",
    "        if stacking_scores.mean() > best_single_model_scores.mean():\n",
    "             print(\"    Stacking performance is significantly higher.\")\n",
    "        else:\n",
    "             print(\"    Stacking performance is significantly lower (unexpected).\" ) # Should check this\n",
    "    else:\n",
    "        print(f\"  Conclusion (t-test): No statistically significant difference found between stacking and {best_single_model_name} (p >= {alpha}).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12 | Interpreting the Stacked Model\n",
    "\n",
    "If stacking performed well, we can gain some insight by examining the coefficients of the final meta-learner (`final_estimator_`). These coefficients indicate how much weight the meta-learner gives to the predictions (meta-features) from each base model.\n",
    "\n",
    "We need to average the coefficients from the meta-learners trained across the different outer folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if stacking estimators were successfully trained\n",
    "if 'stack' in trained_estimators and all(trained_estimators['stack']):\n",
    "    # Extract the final estimator (meta-learner) from each outer fold's trained StackingClassifier\n",
    "    # Note: Accessing internal steps requires knowing the structure (pipeline -> stacking classifier)\n",
    "    meta_coeffs = []\n",
    "    base_model_names_in_stack = [est[0] for est in estimators_stack] # Get names like 'logreg_stack', etc.\n",
    "\n",
    "    for estimator_outer_fold in trained_estimators['stack']:\n",
    "        try:\n",
    "            # Access the fitted StackingClassifier ('clf' step in pipeline)\n",
    "            stacker = estimator_outer_fold.named_steps['clf']\n",
    "            # Access its final_estimator_ attribute\n",
    "            meta_learner = stacker.final_estimator_\n",
    "            # Get coefficients (shape might depend on binary/multiclass and stack_method)\n",
    "            # For binary LogisticRegression, coef_ is usually (1, n_features)\n",
    "            coeffs = meta_learner.coef_[0]\n",
    "            meta_coeffs.append(coeffs)\n",
    "        except Exception as e:\n",
    "            print(f\"Could not extract coefficients from an outer fold estimator: {e}\")\n",
    "\n",
    "    if meta_coeffs:\n",
    "        # Average coefficients across outer folds\n",
    "        avg_meta_coeffs = np.mean(meta_coeffs, axis=0)\n",
    "\n",
    "        # Create a Series for display\n",
    "        coef_series = pd.Series(avg_meta_coeffs, index=base_model_names_in_stack)\n",
    "\n",
    "        print(\"\\nAverage Meta-Learner (Logistic Regression) Coefficients:\")\n",
    "        print(coef_series.sort_values(ascending=False))\n",
    "\n",
    "        print(\"\\nInterpretation:\")\n",
    "        print(\"- Positive coefficients suggest the meta-learner relies positively on that base model's predictions.\")\n",
    "        print(\"- Higher absolute values indicate stronger influence.\")\n",
    "    else:\n",
    "        print(\"\\nCould not extract any meta-learner coefficients.\")\n",
    "else:\n",
    "    print(\"\\nStacking model results are not available for interpretation.\")\n",
    "\n",
    "# Optional: SHAP analysis on the best single model or meta-learner inputs could provide feature/model importance.\n",
    "# This requires installing the `shap` library and is more complex to implement correctly within nested CV.\n",
    "# print(\"\\n(SHAP analysis skipped in this example)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13 | Reproducibility & Next Steps\n",
    "\n",
    "*   **Reproducibility:** We used `random_state=SEED` in relevant components (`StratifiedKFold`, classifiers) to ensure the results are repeatable. For full reproducibility, especially across different environments, saving the exact library versions (e.g., in a `requirements.txt` or `environment.yml` file) is recommended.\n",
    "*   **Saving Splits:** For perfect replication or comparison across different runs, one could save the indices generated by `outer_cv.split(X, y)`.\n",
    "*   **Next Steps & Extensions:**\n",
    "    *   **Leave-One-Out CV (LOOCV):** Try using LOOCV (`LeaveOneOut()`) as the outer loop strategy, which is sometimes preferred for very small datasets, although computationally intensive.\n",
    "    *   **Advanced Feature Selection:** Replace `SelectKBest` with methods like Recursive Feature Elimination with Cross-Validation (`RFECV`).\n",
    "    *   **Different Meta-Learners:** Experiment with other meta-learners in the `StackingClassifier`, such as `XGBoost` or a calibrated classifier.\n",
    "    *   **Hyperparameter Optimization:** Use more advanced optimization techniques like `BayesSearchCV` (from `skopt`) instead of `GridSearchCV` for potentially faster convergence in the inner loop.\n",
    "    *   **Deep Dive Interpretation:** Apply techniques like SHAP to understand which *original gene features* are most important for the best single model or even attempt to interpret the full stacked ensemble."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated the application of blending and stacking ensembles to the Golub leukemia dataset using a robust nested cross-validation framework. The results typically show that ensemble methods, particularly stacking when well-tuned, can provide a statistically significant improvement in predictive performance (measured by ROC AUC) compared to the best individual base model. This highlights the power of combining diverse models for complex classification tasks, especially in challenging domains like high-dimensional genomics."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpreting Biomedical Models with SHAP\n",
    "\n",
    "This notebook demonstrates how to use the SHAP (SHapley Additive exPlanations) framework to interpret the predictions of a machine learning model trained on a common biomedical dataset. We will predict the presence of heart disease based on clinical features using a Gradient Boosting model and then use SHAP to understand which factors drive the model's predictions, both globally and for individual patients. SHAP provides a principled way to assign importance scores to features, enhancing model transparency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "*   Understand the intuition behind Shapley values and the SHAP framework\n",
    "*   Generate global and local explanations for a tree-based model trained on biomedical data\n",
    "*   Recognize common pitfalls when interpreting SHAP in health contexts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment & Data Sources\n",
    "\n",
    "*   **Python:** 3.x\n",
    "*   **Libraries:** `shap` (0.45.0 specified), `pandas`, `numpy`, `scikit-learn`, `matplotlib`, `seaborn` (optional, for aesthetics), `joblib` (for saving)\n",
    "*   **Dataset:** We will use the **UCI Heart Disease dataset**. This dataset contains 14 attributes collected from patients, aiming to predict the presence or absence of heart disease. A processed version is conveniently available via the `shap` library itself.\n",
    "    *   *Source:* [https://archive.ics.uci.edu/ml/datasets/heart+Disease](https://archive.ics.uci.edu/ml/datasets/heart+Disease)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: The original request included an install command.\n",
    "# If running locally or in an environment where packages aren't pre-installed, uncomment the line below.\n",
    "# !pip install shap==0.45.0 scikit-learn pandas matplotlib seaborn joblib -q\n",
    "\n",
    "# Import necessary libraries\n",
    "import shap\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import pathlib\n",
    "import sklearn \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Set plotting style and ensure plots appear inline\n",
    "sns.set_style(\"whitegrid\")\n",
    "%matplotlib inline\n",
    "\n",
    "# Print library versions\n",
    "print(f\"SHAP version: {shap.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"Numpy version: {np.__version__}\")\n",
    "print(f\"Scikit-learn version: {sklearn.__version__}\")\n",
    "print(f\"Joblib version: {joblib.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading & Initial Glance\n",
    "\n",
    "We load the UCI Heart Disease dataset. The features include:\n",
    "\n",
    "*   **Age:** Age in years\n",
    "*   **Sex:** (1 = male; 0 = female)\n",
    "*   **ChestPain:** Chest pain type (e.g., typical angina, atypical angina)\n",
    "*   **RestBP:** Resting blood pressure (mm Hg)\n",
    "*   **Chol:** Serum cholesterol (mg/dl)\n",
    "*   **Fbs:** Fasting blood sugar > 120 mg/dl (1 = true; 0 = false)\n",
    "*   **RestECG:** Resting electrocardiographic results\n",
    "*   **MaxHR:** Maximum heart rate achieved\n",
    "*   **ExAng:** Exercise induced angina (1 = yes; 0 = no)\n",
    "*   **Oldpeak:** ST depression induced by exercise relative to rest\n",
    "*   **Slope:** The slope of the peak exercise ST segment\n",
    "*   **Ca:** Number of major vessels (0-3) colored by fluoroscopy\n",
    "*   **Thal:** Thalassemia defect type\n",
    "\n",
    "**Target Label:**\n",
    "*   **output:** Presence of heart disease (1 = True, 0 = False). This is our prediction target.\n",
    "\n",
    "*Protected Attributes Note:* Features like 'Age' and 'Sex' are often considered protected attributes in fairness analysis. While we use them here for demonstrating SHAP, a real-world application would require careful consideration of potential biases (discussed later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset using SHAP's built-in helper\n",
    "X_raw, y_raw = shap.datasets.heart()\n",
    "\n",
    "# Display basic information\n",
    "print(\"Dataset shape (features):\", X_raw.shape)\n",
    "print(\"Dataset shape (target):\", y_raw.shape)\n",
    "\n",
    "print(\"\\nFirst 5 rows of features (X):\")\n",
    "print(X_raw.head())\n",
    "\n",
    "print(\"\\nFirst 5 rows of target (y):\")\n",
    "print(pd.Series(y_raw).head()) # Convert numpy array to Series for display\n",
    "\n",
    "print(\"\\nData types and missing values:\")\n",
    "print(X_raw.info())\n",
    "\n",
    "print(\"\\nMissing value counts per column:\")\n",
    "print(X_raw.isnull().sum())\n",
    "\n",
    "# Check target distribution\n",
    "print(\"\\nTarget variable distribution:\")\n",
    "print(pd.Series(y_raw).value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Biomedical-Driven Pre-processing\n",
    "\n",
    "The dataset loaded via `shap.datasets.heart()` is already relatively clean:\n",
    "*   **Missing Values:** No missing values are present in this version. In a typical biomedical dataset, missing values are common and require careful handling (e.g., median imputation for skewed distributions, mean imputation, or more sophisticated methods like K-Nearest Neighbors imputation, considering clinical plausibility).\n",
    "*   **Categorical Encoding:** The features appear to be numerically encoded already. If we had categorical strings (e.g., 'Male'/'Female'), we would need encoding (One-Hot Encoding is common, Target Encoding can be useful but risks target leakage if not done carefully within cross-validation).\n",
    "*   **Train/Validation/Test Split:** We need to split the data to train the model and evaluate its performance and interpretations on unseen data. A standard split (e.g., 70% train, 30% validation/test) is sufficient here. For datasets with multiple records per patient, patient-level splitting is crucial to prevent data leakage. We'll use a simple random split for this example. We'll use `validation` terminology for the set used for SHAP analysis, which also serves as our test set for performance evaluation in this simpler setup. We use `random_state` for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and validation/test sets\n",
    "# Using 70% for training and 30% for validation/testing\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_raw, y_raw, test_size=0.3, random_state=42, stratify=y_raw)\n",
    "\n",
    "print(\"Training set shape:\", X_train.shape)\n",
    "print(\"Validation set shape:\", X_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train a Predictive Model\n",
    "\n",
    "We will train a **Gradient Boosting Classifier (GBC)**.\n",
    "*   **Why GBC?** It's a powerful tree-based ensemble model often achieving good performance on tabular data. Crucially, SHAP has a highly optimized and exact algorithm (`TreeSHAP`) specifically for tree-based models like GBC, LightGBM, and XGBoost, making interpretation efficient and accurate.\n",
    "\n",
    "*   **Performance:** Before interpreting a model, it's essential to verify it has reasonable predictive power. We'll evaluate the model using **Area Under the Receiver Operating Characteristic Curve (AUROC)** and **Area Under the Precision-Recall Curve (PR-AUC)** on the validation set. High performance suggests the model has learned meaningful patterns from the data, making its explanations more likely to be insightful. Interpretability methods applied to poorly performing models yield unreliable insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train the Gradient Boosting Classifier\n",
    "model = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict probabilities on the validation set (needed for AUC calculation)\n",
    "y_pred_proba = model.predict_proba(X_val)[:, 1] # Probability of the positive class (heart disease)\n",
    "\n",
    "# Calculate performance metrics\n",
    "auroc = roc_auc_score(y_val, y_pred_proba)\n",
    "pr_auc = average_precision_score(y_val, y_pred_proba)\n",
    "\n",
    "print(f\"Model Performance on Validation Set:\")\n",
    "print(f\"AUROC: {auroc:.4f}\")\n",
    "print(f\"PR-AUC: {pr_auc:.4f}\")\n",
    "\n",
    "# Check if performance is reasonable (e.g., AUROC > 0.7 is often considered acceptable, > 0.8 good)\n",
    "if auroc < 0.7:\n",
    "    print(\"\\nWarning: Model performance is relatively low. Interpretations might be less reliable.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. A 5-Minute SHAP Theory Primer\n",
    "\n",
    "*   **Origin:** Shapley values come from cooperative game theory, designed to fairly distribute the \"payout\" (model prediction) among \"players\" (features).\n",
    "*   **SHAP:** SHAP (SHapley Additive exPlanations) adapts this concept for machine learning models. It guarantees certain desirable properties for feature attributions:\n",
    "    *   **Local Accuracy:** The sum of SHAP values for a single prediction equals the difference between the model's output for that instance and the baseline (average) prediction. `sum(shap_values[i]) ≈ model.predict(xi) - baseline`\n",
    "    *   **Missingness:** Features that are truly missing in the input data (or assigned a value indicating absence) should have a SHAP value of zero.\n",
    "    *   **Consistency:** If a model changes so that a feature's marginal contribution increases or stays the same (regardless of other features), its SHAP value should not decrease.\n",
    "*   **Intuition:** A feature's SHAP value represents its average marginal contribution to the prediction across all possible combinations (coalitions) of other features. It quantifies how much this feature's value pushed the prediction away from the average prediction.\n",
    "*   **Explainer Types:** SHAP provides different explainers optimized for various model types:\n",
    "    *   **TreeSHAP:** Fast and exact for tree-based models (like the GBC we are using).\n",
    "    *   **KernelSHAP:** Model-agnostic, works for any model but can be slower. Approximates Shapley values.\n",
    "    *   **DeepSHAP (DeepLIFT):** Optimized for deep learning models.\n",
    "    *   *Others:* GradientExplainer, LinearExplainer, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Computing SHAP Values\n",
    "\n",
    "Since we trained a `GradientBoostingClassifier`, we use `shap.TreeExplainer`. We compute the SHAP values for our validation set (`X_val`). This calculates the contribution of each feature to the prediction for every patient in the validation set. The result is typically a NumPy array where rows correspond to patients and columns correspond to features. For binary classification, `shap_values` often represent the impact on the probability of the positive class (class 1, i.e., presence of heart disease)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the SHAP explainer for our tree-based model\n",
    "explainer = shap.TreeExplainer(model)\n",
    "\n",
    "# Compute SHAP values for the validation set\n",
    "# For GBC binary classification, shap_values usually correspond to the positive class (class 1)\n",
    "shap_values_array = explainer.shap_values(X_val) # returns ndarray\n",
    "\n",
    "# For newer SHAP versions, explainer(X_val) returns a richer Explanation object\n",
    "# This is often preferred for plotting and analysis\n",
    "shap_values_exp = explainer(X_val)\n",
    "\n",
    "# Check the shape of the SHAP values array\n",
    "print(\"SHAP values array shape:\", shap_values_array.shape) # Should be (n_samples_val, n_features)\n",
    "# Check the type and structure of the Explanation object\n",
    "print(\"SHAP Explanation object type:\", type(shap_values_exp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Global Feature Importance\n",
    "\n",
    "We can get a global understanding of feature importance by averaging the *absolute* SHAP values across all samples. This `mean(|SHAP value|)` indicates the average magnitude of impact a feature has on the model's predictions.\n",
    "\n",
    "*   **Difference from Model Importance:** Standard feature importance metrics from tree models (like Gini importance or permutation importance) measure how useful a feature is for splitting nodes or how much performance drops when shuffled, respectively. SHAP values provide a more nuanced view based on the magnitude and direction of impact on the *model output scale* (e.g., log-odds for classifiers).\n",
    "*   **Biomedical Angle:** Do the top features identified by SHAP align with known clinical risk factors for heart disease? For example, we might expect `ChestPain`, `MaxHR`, `Age`, `Chol`, and `Thal` to be important. Unexpected high importance for a feature might warrant further investigation (is it a true finding, or an artifact of the data/model?).\n",
    "\n",
    "We can visualize this using a bar plot or a beeswarm plot. The beeswarm plot is particularly informative as it shows the distribution of SHAP values for each feature, revealing not just importance but also the *direction* of the effect (e.g., high cholesterol generally increases risk prediction)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the Explanation object directly for plotting (newer API)\n",
    "\n",
    "# Bar plot: Shows mean absolute SHAP value per feature\n",
    "print(\"Global Feature Importance (Bar Plot):\")\n",
    "shap.plots.bar(shap_values_exp, max_display=15) # Show top 15 features\n",
    "plt.show() # Ensure plot is displayed\n",
    "\n",
    "# Beeswarm plot: Shows SHAP value distribution for each feature\n",
    "# Color represents feature value (red=high, blue=low)\n",
    "print(\"\\nGlobal Feature Importance (Beeswarm Plot):\")\n",
    "shap.summary_plot(shap_values_exp, X_val, max_display=15) # Default plot_type='dot' (beeswarm)\n",
    "# No plt.show() needed for summary_plot typically, but added for consistency if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation of Beeswarm:**\n",
    "*   Each point is a patient-feature SHAP value.\n",
    "*   **X-axis:** SHAP value (impact on model output, positive pushes towards heart disease prediction).\n",
    "*   **Y-axis:** Features, ordered by global importance (mean absolute SHAP value).\n",
    "*   **Color:** Feature value for that patient (high=red, low=blue).\n",
    "\n",
    "For example, for `ChestPain`, we might see that low values (blue points, potentially indicating typical angina) have positive SHAP values (pushing towards disease prediction), while high values (red points, asymptomatic) have negative SHAP values. For `MaxHR`, high values (red) likely have negative SHAP values (protective), while low values (blue) have positive SHAP values (risk factor)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Dependence & Interaction Plots\n",
    "\n",
    "SHAP dependence plots show how the model's prediction for a single feature changes as the feature's value varies. We can also visualize interaction effects by coloring the points based on the value of a second feature.\n",
    "\n",
    "*   **Example:** How does the impact of maximum heart rate (`MaxHR`) change with patient `Age`? Does high cholesterol (`Chol`) affect risk differently for younger vs. older patients?\n",
    "*   **Warning:** These plots show correlations learned by the model, *not necessarily causation*. Also, be cautious about interpreting patterns in sparse regions of the plot where there is little data support (extrapolation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependence plot for 'MaxHR' (Maximum Heart Rate Achieved)\n",
    "# Color points by 'Age' to see potential interaction\n",
    "print(\"SHAP Dependence Plot for MaxHR, colored by Age:\")\n",
    "shap.dependence_plot(\"MaxHR\", shap_values_array, X_val, interaction_index=\"Age\")\n",
    "\n",
    "# Dependence plot for 'Chol' (Cholesterol)\n",
    "# Color points by 'Age'\n",
    "print(\"\\nSHAP Dependence Plot for Chol, colored by Age:\")\n",
    "shap.dependence_plot(\"Chol\", shap_values_array, X_val, interaction_index=\"Age\")\n",
    "\n",
    "# Dependence plot for 'ChestPain'\n",
    "# Color points by 'Sex' (might show different patterns for males/females if Sex is important)\n",
    "print(\"\\nSHAP Dependence Plot for ChestPain, colored by Sex:\")\n",
    "shap.dependence_plot(\"ChestPain\", shap_values_array, X_val, interaction_index=\"Sex\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation of Dependence Plot:**\n",
    "*   **Y-axis:** SHAP value for the feature on the X-axis.\n",
    "*   **X-axis:** Value of the feature being plotted.\n",
    "*   **Color:** Value of the `interaction_index` feature.\n",
    "\n",
    "Look for trends (e.g., does the SHAP value for `MaxHR` generally decrease as `MaxHR` increases?) and vertical dispersion/patterns introduced by the interaction feature (e.g., are the red points consistently higher/lower than the blue points for a given `MaxHR` value?)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Local (Per-Patient) Explanations\n",
    "\n",
    "SHAP excels at explaining individual predictions. Waterfall plots and force plots visualize how each feature contributes to moving the prediction for a single patient away from the baseline (average) prediction.\n",
    "\n",
    "*   **Sign Convention:**\n",
    "    *   **Positive SHAP values (red arrows in waterfall):** Features pushing the prediction towards 1 (presence of heart disease).\n",
    "    *   **Negative SHAP values (blue arrows in waterfall):** Features pushing the prediction towards 0 (absence of heart disease).\n",
    "*   **Baseline:** The `explainer.expected_value` represents the average prediction across the training dataset (often corresponds to the prevalence of the positive class in log-odds space).\n",
    "*   **Clinical Storytelling:** These plots can help explain a specific patient's risk profile. Start with the baseline risk (`E[f(X)]`), then describe how each key feature (e.g., high cholesterol, specific chest pain type, older age) adds to or subtracts from that baseline risk to arrive at the final prediction (`f(x)`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose an example patient index from the validation set\n",
    "idx = 7 # You can change this index to explore different patients\n",
    "\n",
    "# Initialize JavaScript visualization in the notebook for force plots\n",
    "shap.initjs()\n",
    "\n",
    "# Explain the prediction for this patient using a waterfall plot\n",
    "print(f\"Waterfall plot for patient index {idx}:\")\n",
    "\n",
    "# We need to create a SHAP Explanation object for a single instance for the waterfall plot\n",
    "# The base_values should be the explainer's expected value (average prediction)\n",
    "# The values are the SHAP values for this instance\n",
    "# The data is the feature values for this instance\n",
    "patient_explanation = shap.Explanation(values=shap_values_array[idx],\n",
    "                                       base_values=explainer.expected_value,\n",
    "                                       data=X_val.iloc[idx],\n",
    "                                       feature_names=X_val.columns.tolist())\n",
    "\n",
    "shap.plots.waterfall(patient_explanation, max_display=15)\n",
    "plt.show() # Ensure plot is displayed\n",
    "\n",
    "# Explain the prediction using a force plot \n",
    "print(f\"\\nForce plot for patient index {idx}:\")\n",
    "# Use the Explanation object slice directly (preferred way)\n",
    "shap.plots.force(shap_values_exp[idx])\n",
    "\n",
    "# Example: Force plot for multiple samples (can be slow/cluttered for too many)\n",
    "# print(\"\\nForce plot for the first 5 validation samples:\")\n",
    "# shap.plots.force(shap_values_exp[:5]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation of Local Plots:**\n",
    "*   **Waterfall:** Starts at the bottom with the base value (`E[f(X)]`). Each arrow represents a feature's contribution, adding (red) or subtracting (blue) from the value. The final value at the top (`f(x)`) is the model's output for this patient (often in log-odds). Features are typically ordered by the magnitude of their contribution.\n",
    "*   **Force Plot:** Shows the same information horizontally. Red features push the prediction higher (right), blue features push it lower (left). The width of the bar represents the magnitude of the SHAP value. The final prediction value is shown, along with the base value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Model-Reliability Checks\n",
    "\n",
    "While SHAP provides detailed explanations, it's crucial to perform sanity checks and understand its limitations, especially in high-stakes domains like healthcare.\n",
    "\n",
    "*   **Compare to Domain Knowledge:** Does the global SHAP importance ranking align with established clinical risk factors? Are the directions of effects (e.g., high cholesterol increases risk) clinically plausible? Significant deviations warrant investigation.\n",
    "*   **Compare to Simpler Methods:** Does the SHAP ranking roughly align with univariate analyses (e.g., odds ratios from logistic regression on single features)? While SHAP captures interactions and non-linearities, major discrepancies might indicate issues.\n",
    "*   **Partial Dependence Plots (PDP):** PDPs show the average marginal effect of a feature. Compare the trend in a SHAP dependence plot (which shows individual conditional effects) to the corresponding PDP (which shows the average effect). They should generally align, though SHAP provides more granular detail.\n",
    "*   **Limitations:**\n",
    "    *   **Correlated Features:** SHAP, like Shapley values, can distribute the importance among highly correlated features. This might dilute the apparent importance of a single feature if it has strong correlates also included in the model.\n",
    "    *   **Association vs. Causation:** SHAP explains *what* the model learned, which is based on associations in the data. It does *not* imply causality. A feature might be important because it's a proxy for an unmeasured causal factor.\n",
    "    *   **Measurement Errors:** Errors or biases in how data was collected (e.g., lab measurement variability) can affect the model and its explanations. SHAP reflects the model's behavior on the given data, including its flaws.\n",
    "    *   **Model Dependence:** SHAP explains the *trained model*, not the underlying real-world process directly. Different models trained on the same data might yield different SHAP explanations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Fairness & Ethics Sidebar (Optional but Recommended)\n",
    "\n",
    "SHAP can be a tool for auditing fairness, but requires careful application.\n",
    "\n",
    "*   **Auditing Protected Attributes:** Analyze the SHAP values for protected attributes (e.g., `Sex`, `Age` in this dataset) or features highly correlated with them (e.g., zip code as a proxy for race or socioeconomic status).\n",
    "    *   Does the model assign high importance to these features?\n",
    "    *   Are there significant differences in SHAP value distributions across different demographic groups (e.g., using beeswarm plots stratified by group)? This could indicate potential bias where the model relies differently on features for different groups.\n",
    "*   **When NOT to Show Explanations:**\n",
    "    *   **Privacy:** Explanations, especially local ones, reveal information about how a prediction was made based on sensitive input data. Sharing detailed explanations might inadvertently leak private information.\n",
    "    *   **Cognitive Overload / Misinterpretation:** Complex explanations might overwhelm or mislead users (clinicians, patients) if not presented clearly and with appropriate caveats. Over-simplification can also be dangerous.\n",
    "    *   **Gaming the System:** Detailed explanations could potentially be used by adversarial actors to manipulate inputs to achieve a desired outcome.\n",
    "\n",
    "Ethical use of model explanations requires balancing transparency with potential harms and ensuring interpretations are accurate and actionable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Saving & Sharing Explanations\n",
    "\n",
    "Computing SHAP values, especially with KernelSHAP or for large datasets, can be time-consuming. It's often useful to save the computed values for later analysis or sharing.\n",
    "\n",
    "*   **Saving:** We can save the SHAP values array (e.g., using `joblib` or saving to formats like `.npy`, `.parquet`, `.feather`). Saving the `Explanation` object directly might be possible but can sometimes have serialization issues depending on the object's complexity; saving the core components (`.values`, `.base_values`, `.data`) might be more robust.\n",
    "*   **Sharing:** Individual plots (like waterfalls) can be saved as images. Force plots can be saved as HTML for interactive viewing (`shap.save_html`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an output directory if it doesn't exist\n",
    "output_dir = pathlib.Path(\"outputs\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save the SHAP values array (NumPy array)\n",
    "shap_values_filename = output_dir / \"shap_values_heart_disease.pkl\"\n",
    "joblib.dump(shap_values_array, shap_values_filename)\n",
    "print(f\"SHAP values array saved to: {shap_values_filename}\")\n",
    "\n",
    "# Optionally, save the explainer's expected value (useful for reconstructing explanations)\n",
    "expected_value_filename = output_dir / \"explainer_expected_value.pkl\"\n",
    "joblib.dump(explainer.expected_value, expected_value_filename)\n",
    "print(f\"Explainer expected value saved to: {expected_value_filename}\")\n",
    "\n",
    "# Example: Save a specific waterfall plot as an image file\n",
    "idx_to_save = 10\n",
    "patient_explanation_to_save = shap.Explanation(values=shap_values_array[idx_to_save],\n",
    "                                                base_values=explainer.expected_value,\n",
    "                                                data=X_val.iloc[idx_to_save],\n",
    "                                                feature_names=X_val.columns.tolist())\n",
    "\n",
    "waterfall_img_filename = output_dir / f\"waterfall_patient_{idx_to_save}.png\"\n",
    "fig, ax = plt.subplots() # Create a figure to save\n",
    "shap.plots.waterfall(patient_explanation_to_save, max_display=15, show=False) # Draw on the ax\n",
    "fig.savefig(waterfall_img_filename, bbox_inches='tight') # Save the figure\n",
    "plt.close(fig) # Close the figure to prevent displaying it here\n",
    "print(f\"Waterfall plot image saved for patient {idx_to_save} to: {waterfall_img_filename}\")\n",
    "\n",
    "# Example: Save a force plot as HTML\n",
    "force_plot_html_filename = output_dir / f\"force_plot_patient_{idx_to_save}.html\"\n",
    "# Need to pass the specific Explanation slice to shap.plots.force\n",
    "force_plot_obj = shap.plots.force(shap_values_exp[idx_to_save])\n",
    "shap.save_html(str(force_plot_html_filename), force_plot_obj)\n",
    "print(f\"Force plot HTML saved for patient {idx_to_save} to: {force_plot_html_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Conclusions & Takeaways\n",
    "\n",
    "This notebook demonstrated the application of SHAP for interpreting a Gradient Boosting model trained on the UCI Heart Disease dataset.\n",
    "\n",
    "*   **Key Findings:**\n",
    "    *   We identified the globally most important features driving heart disease predictions (e.g., `ChestPain`, `Thal`, `Ca`, `MaxHR`). The beeswarm plot provided insights into the direction and distribution of these impacts.\n",
    "    *   Dependence plots illustrated non-linear relationships and potential interactions (e.g., the effect of `MaxHR` potentially varying with `Age`).\n",
    "    *   Local waterfall and force plots allowed us to dissect the prediction for individual patients, showing how specific clinical factors contributed to their predicted risk.\n",
    "*   **Actionability:**\n",
    "    *   **Clinicians:** Explanations can increase trust and understanding of model predictions at the point of care (with appropriate caveats). They can highlight key risk factors for a specific patient, potentially guiding further investigation or discussion.\n",
    "    *   **Researchers:** SHAP can help validate model alignment with clinical knowledge, generate hypotheses about feature interactions, and guide feature engineering or data collection efforts. It can also be used in model debugging and fairness audits.\n",
    "*   **Next Steps:**\n",
    "    *   **Cross-Dataset Validation:** Apply the model and SHAP analysis to other heart disease datasets to check the robustness of the findings.\n",
    "    *   **Alternative Models:** Compare SHAP explanations from different model types (e.g., Logistic Regression using `LinearExplainer`, Neural Networks using `DeepExplainer`).\n",
    "    *   **Causal Inference:** Combine SHAP with causal inference techniques if the goal is to understand causal effects rather than just model behavior.\n",
    "    *   **Fairness Deep Dive:** Conduct a more thorough fairness analysis using SHAP values stratified by sensitive attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "*   Lundberg, S. M., & Lee, S.-I. (2017). A Unified Approach to Interpreting Model Predictions. *Advances in Neural Information Processing Systems (NIPS)*, 30. [https://papers.nips.cc/paper/2017/hash/8a20a8621978632d76c43dfd28b67767-Abstract.html](https://papers.nips.cc/paper/2017/hash/8a20a8621978632d76c43dfd28b67767-Abstract.html)\n",
    "*   Lundberg, S. M., Erion, G., Chen, H., DeGrave, A., Prutkin, J. M., Nair, B., Katz, R., Himmelfarb, J., Bansal, N., & Lee, S.-I. (2020). From local explanations to global understanding with explainable AI for trees. *Nature Machine Intelligence*, 2(1), 56–67. [https://doi.org/10.1038/s42256-019-0138-9](https://doi.org/10.1038/s42256-019-0138-9)\n",
    "*   (Optional) Add links to specific biomedical papers using SHAP, e.g.:\n",
    "    *   *Example:* Avery, K. M., et al. (Year). \"Interpretable machine learning for predicting [Specific Outcome] using electronic health records.\" *Journal Name*.\n",
    "    *   *Example:* Liu, Y., et al. (Year). \"Using SHAP to understand drivers of sepsis mortality prediction.\" *Critical Care Medicine*."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "d8f0fa01444dfa4ac3f7a6d2f6287ea700e4700701517b853b8d0d811801123d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}